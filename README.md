# LLM Content Moderation Analysis Platform ğŸ›¡ï¸

A comprehensive research platform for auditing and analyzing how Large Language Models handle content moderation decisions.

**Live Dashboard**: [llm-content-moderation-analysis.vercel.app](https://llm-content-moderation-analysis.vercel.app)

## âœ¨ Features

### Dashboard
- **Overview** - Real-time stats, heatmaps, model comparison, and audit logs
- **Compare** - Side-by-side model comparison with radar charts
- **Deep Dive** - Statistical analysis, semantic clustering, **Bias Compass**, and **Model Registry**
- **Efficiency** - Cost vs. Safety trade-offs

### Backend
- **Multi-Model Auditing** - Test OpenAI, Anthropic, Google, and open-source models
- **Bias Analysis** - Quadrant mapping of refusal reasoning (Left/Right/Auth/Lib)
- **Statistical Analysis** - Fleiss Kappa, Agreement Distribution
- **Cost Tracking** - Per-model cost calculation

## ğŸš€ Quick Start

### Option A: Docker (Recommended)
```bash
# 1. Configure API Keys
echo "OPENROUTER_API_KEY=sk-or-v1-..." > .env

# 2. Run the Stack
docker-compose up --build
```
- **Frontend**: http://localhost:3000
- **Backend**: Running in background

### Option B: Local Setup
```bash
# Backend (Python)
pip install -r requirements.txt
python src/migrate_csv_to_sql.py  # Initialize DB

# Frontend (Next.js)
cd web && npm install && npm run dev
```

## ğŸ“Š Running Audits

### Manual Audit
```bash
# Test specific models
python src/audit_runner.py --model openai/gpt-4o-mini

# Use presets
python src/audit_runner.py --preset low     # Efficiency tier
python src/audit_runner.py --preset mid     # Medium tier
python src/audit_runner.py --preset high    # Expensive tier

# Flags
# --force        Ignore cache (default: 7-day cache)
# --limit N      Run only N prompts
# --policy v1.0  Tag for A/B testing
```

### Scheduled Audits (GitHub Actions)
| Schedule              | Models                          |
|----------------------|--------------------------------|
| Weekly (Sundays)     | GPT-4o-mini, Claude Haiku, etc |
| Monthly (1st)        | Gemini Flash, Large Models     |

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ src/                  # Python backend
â”‚   â”œâ”€â”€ audit_runner.py   # Main auditing script
â”‚   â”œâ”€â”€ statistics.py     # Statistical analysis
â”‚   â””â”€â”€ cluster_analysis.py
â”œâ”€â”€ web/                  # Next.js dashboard
â”‚   â”œâ”€â”€ app/              # Pages (dashboard, compare, analysis)
â”‚   â””â”€â”€ components/       # React components
â”œâ”€â”€ data/                 # Prompts and model configs
â”‚   â”œâ”€â”€ prompts.csv       # Test prompts by category
â”‚   â””â”€â”€ models.json       # Model registry
â”œâ”€â”€ .github/workflows/    # CI/CD and scheduled audits
â””â”€â”€ tests/                # Integration tests
```

## ğŸ› ï¸ Deployment

**Frontend (Vercel)**: Auto-deploys on push to main
**GitHub Actions**: Handles scheduled model auditing

## ğŸ“œ License
Internal Research Tool - MIT License
