# LLM Content Moderation Analysis Platform ğŸ›¡ï¸

![CI](https://github.com/jmk9494-code/llm-content-moderation-analysis/actions/workflows/ci.yml/badge.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

A comprehensive research platform for auditing and analyzing how Large Language Models handle content moderation decisions.

**Live Dashboard**: [llm-content-moderation-analysis.vercel.app](https://llm-content-moderation-analysis.vercel.app)

## âœ¨ Features

### Dashboard
- **Overview** - Real-time stats, heatmaps, interactive model filtering, and audit logs
- **Compare** - Side-by-side model comparison with statistical significance tests
- **Analysis** - **Bias Compass** (All Models), Longitudinal Study (Multi-model support), and **Weekly AI Analyst** reports
- **Efficiency** - Cost vs. Refusal Rate trade-offs
- **Export** - Download comparison data as CSV

### Phase 8 Upgrades (New)
- **Political Compass** ğŸ§­ - Models mapped on Economic/Social axes (Authority vs Liberty).
- **Paternalism Detection** ğŸ‘¶ - Measures if models treat "Teenagers" differently than "Researchers".
- **Evidence Locker** ğŸ“‚ - Full transparency explorer for raw audit traces.
- **Automated Benchmarks** ğŸ¤– - Self-updating weekly audits via GitHub Actions.

### Backend
- **Multi-Model Auditing** - Test OpenAI, Anthropic, Google, and open-source models
- **Bias Analysis** - Quadrant mapping of refusal reasoning (Left/Right/Auth/Lib)
- **Statistical Analysis** - Fleiss Kappa, Refusal Rate Agreement
- **Cost Tracking** - Per-model cost calculation

## ğŸš€ Quick Start

### Option A: Docker (Recommended)
```bash
# 1. Configure API Keys
echo "OPENROUTER_API_KEY=sk-or-v1-..." > .env

# 2. Run the Stack
docker-compose up --build
```
- **Frontend**: http://localhost:3000
- **Backend**: Running in background

### Option B: Local Setup
```bash
# Backend (Python)
pip install -r requirements.txt
python src/migrate_csv_to_sql.py  # Initialize DB

# Frontend (Next.js)
cd web && npm install && npm run dev
```

## ğŸ“Š Running Audits

### Manual Audit
```bash
# Test specific models
python src/audit_runner.py --model openai/gpt-4o-mini

# Use presets
python src/audit_runner.py --preset low     # Efficiency tier
python src/audit_runner.py --preset mid     # Medium tier
python src/audit_runner.py --preset high    # Expensive tier

# Flags
# --force        Ignore cache (default: 7-day cache)
# --limit N      Run only N prompts
# --policy v1.0  Tag for A/B testing
```

### Scheduled Audits (GitHub Actions)
| Tier       | Schedule                       | Models (Type)                   |
|------------|--------------------------------|---------------------------------|
| Efficiency | Weekly (Sundays)               | Low Cost / Fast (e.g. Haiku)    |
| Medium     | Monthly (1st)                  | Balanced Models                 |
| Expensive  | Bi-Monthly (Jan, Mar...)       | High Intelligence / Large       |

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ src/                  # Python backend
â”‚   â”œâ”€â”€ audit_runner.py   # Main auditing script
â”‚   â”œâ”€â”€ analyst.py        # AI Analysis agent
â”‚   â”œâ”€â”€ analyze_bias.py   # Bias compass calculation
â”‚   â”œâ”€â”€ statistics.py     # Statistical analysis
â”‚   â”œâ”€â”€ policy_tuner.py   # Policy optimization
â”‚   â””â”€â”€ migrate_csv_to_sql.py # DB initialization
â”œâ”€â”€ web/                  # Next.js dashboard
â”‚   â”œâ”€â”€ app/              # App Router (dashboard, compare, analysis)
â”‚   â”œâ”€â”€ components/       # React components
â”‚   â””â”€â”€ public/           # Static assets
â”œâ”€â”€ data/                 # Prompts and model configs
â”‚   â”œâ”€â”€ prompts.csv       # Test prompts by category
â”‚   â””â”€â”€ models.json       # Model registry
â”œâ”€â”€ .github/workflows/    # CI/CD and scheduled audits
â””â”€â”€ tests/                # Unit and integration tests
```


## ğŸ§ª Testing

Run the test suite to verify statistical logic and backend functionality:
```bash
python -m pytest tests/
```

## ğŸ’» Development

We use `ruff` for linting and `black` for formatting:
```bash
# Linting
ruff check .

# Formatting
black .
```

## ğŸ¤ Contributing

When adding new features, please ensure you:
1.  Run the test suite (`python -m pytest tests/`).
2.  Add regression tests for any bug fixes (see `tests/test_audit_runner.py` for examples).

## ğŸ› ï¸ Deployment

**Frontend (Vercel)**: Auto-deploys on push to main
**GitHub Actions**: Handles scheduled model auditing

## ğŸ“œ License
Internal Research Tool - MIT License
