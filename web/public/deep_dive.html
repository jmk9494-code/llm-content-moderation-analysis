<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Technical Deep Dive | LLM Censorship Analysis</title>
    <link rel="stylesheet" href="css/deep_dive.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Merriweather:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
</head>
<body>
    <header class="deep-dive-header">
        <div class="container">
            <h1>Technical Deep Dive</h1>
            <p class="subtitle">Forensic analysis of LLM censorship patterns and alignment taxonomies.</p>
            <a href="/" class="back-link">‚Üê Return to Dashboard</a>
        </div>
    </header>

    <main class="container">
        <section id="academic-figures" class="dashboard-grid">
            
            <!-- Figure 1: Pareto -->
            <div class="chart-card">
                <div class="card-header">
                    <h3>Figure 1: The Alignment Tax</h3>
                    <span class="badge analysis">Pareto Analysis</span>
                </div>
                <div class="viz-container">
                    <iframe src="assets/pareto.html" loading="lazy" title="Alignment Tax Scatter Plot"></iframe>
                </div>
                <p class="caption">
                    <strong>Safety vs. Utility.</strong> Models in the top-right quadrant exhibit "Over-censorship" (High Safety on harmful prompts, but Low Utility on benign prompts). The ideal model resides in the top-left (High Utility, High Safety).
                </p>
            </div>

            <!-- Figure 2: Heatmap -->
            <div class="chart-card">
                <div class="card-header">
                    <h3>Figure 2: Censorship Fingerprint</h3>
                    <span class="badge forensic">Topic Heatmap</span>
                </div>
                <div class="viz-container">
                    <!-- Displaying PDF in iframe is standard for this type of dense report asset -->
                    <iframe src="assets/heatmap.pdf" loading="lazy" title="Censorship Heatmap"></iframe>
                </div>
                <p class="caption">
                    <strong>Topic Bias.</strong> Red zones indicate systematic censorship on specific categories. Note the variance between models on political categories compared to universal refusal on "Dangerous Content".
                </p>
            </div>

            <!-- Figure 3: Clusters -->
            <div class="chart-card full-width">
                <div class="card-header">
                    <h3>Figure 3: Semantic Clusters ("The Island of Censorship")</h3>
                    <span class="badge nlp">Embedding Space</span>
                </div>
                <div class="viz-container large">
                    <iframe src="assets/semantic_clusters.html" loading="lazy" title="Semantic Cluster Map"></iframe>
                </div>
                <p class="caption">
                    <strong>The Shape of Refusal.</strong> t-SNE/UMAP projection of refusal vectors. "Red Islands" reveal broad conceptual blind spots where models systematically refuse adjacent topics.
                </p>
            </div>

            <!-- Figure 4: Word Cloud -->
            <div class="chart-card">
                <div class="card-header">
                    <h3>Figure 4: The "Trigger List"</h3>
                    <span class="badge forensic">Vocabulary Analysis</span>
                </div>
                <div class="viz-container">
                    <img src="assets/wordcloud.png" alt="Trigger Word Cloud" loading="lazy">
                </div>
                <p class="caption">
                    <strong>Forbidden Vocabulary.</strong> Terms most frequently associated with 'Hard Refusal' responses. Size corresponds to frequency in refused prompts.
                </p>
            </div>

        </section>

        <section id="methodology" class="text-section">
            <h2>Methodology</h2>
            <p>
                This analysis utilizes a multi-stage audit pipeline. We evaluate models against 
                <strong>Standardized Benchmarks (XSTest)</strong> for false refusals and a custom 
                <strong>Sensitive Topics</strong> dataset for true refusals.
            </p>
            <p>
                <strong>Metrics:</strong>
                <ul>
                    <li><em>False Refusal Rate (FRR):</em> Percentage of benign prompts refused.</li>
                    <li><em>True Refusal Rate (TRR):</em> Percentage of harmful prompts refused.</li>
                    <li><em>Preachiness Score:</em> Semantic density of moralizing language in compliant answers.</li>
                </ul>
            </p>
            <div class="links">
                <a href="https://github.com/jmk9494-code/llm-content-moderation-analysis" target="_blank">View Source Code</a>
                &bull;
                <a href="assets/heatmap.pdf" target="_blank">Download Full Report (PDF)</a>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 LLM Content Moderation Analysis Project.</p>
    </footer>
</body>
</html>
