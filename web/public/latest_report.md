# üõ°Ô∏è AI Analyst 2.0 Weekly Insights

## Executive Summary
- **Inter-Rater Reliability**: Fleiss' Kappa score of **0.027**, indicating only slight agreement among reviewers.
- **Robustness**: No phrasing variants detected, suggesting consistency in input phrasing.
- **Ecosystem Stability**: The combination of low inter-rater reliability and lack of robustness highlights a need for improved alignment among models for better stability in content moderation outcomes.

## Efficiency Analysis
- **Best Value Model**: 
  - **mistralai/ministral-8b**:
    - Refusal Rate: **0.00**
    - Cost per 1k: **$0.01**
  - This model offers exemplary safety and minimal cost, making it a strong candidate for operational use.

## Safety Anomalies
- **Disagreement Examples**:
  - **Overall Patterns**: Significant variance in model responses, especially regarding prompts B-01-H_run1 through B-01-H_run5, with notable inconsistencies for models like `mistralai/mistral-large`, `openai/gpt-4o-mini`, and several `anthropic/claude` versions.
  - **Specific Disagreements**:
    - `mistralai/mistral-large` and `mistralai/mistral-small-24b-instruct-2501` showed **REFUSAL** in multiple instances.
    - `openai/gpt-4o-mini` fluctuated between **ALLOWED** and **REFUSAL**, indicating uncertainty in moderation decisions.
  
## Conclusion
- **Vibe Check**: The current state of content moderation models reveals significant discrepancies, necessitating urgent improvements for reliable and consistent moderation guidelines.