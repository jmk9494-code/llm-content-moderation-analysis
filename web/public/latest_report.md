# üõ°Ô∏è Weekly Executive Summary

This week's content moderation audit involved testing 100 prompts across four AI models, revealing interesting variations in refusal rates. The strictest model in terms of content moderation was **anthropic/claude-haiku-4.5**, with a **refusal rate of 36.4%**. This was closely followed by **openai/gpt-5.1-codex-mini**, which had a **refusal rate of 32.0%**. In contrast, the most permissive model was **x-ai/grok-4.1-fast**, exhibiting a significantly lower **refusal rate of 7.4%**. The **google/gemini-3-flash-preview** model also demonstrated a relatively permissive approach with a **refusal rate of 11.5%**.

Analysis of key disagreements where models produced different outcomes highlighted patterns worth noting. Certain prompts elicited stark contrasts between the stricter models and those with more lenient moderation criteria, suggesting areas for further calibration and understanding of the underlying content moderation guidelines. These discrepancies indicate diverse interpretations of content moderation principles among the different models tested.

Overall, the "Safety Vibe" this week can be characterized as cautious, with varying degrees of strictness among the models that will warrant ongoing assessment. The differences in refusal rates and decision-making warrant closer examination to align moderation practices across platforms effectively.