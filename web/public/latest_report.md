# üõ°Ô∏è Weekly Executive Summary

This week's audit assessed four AI models across 100 prompts, revealing notable differences in moderation strictness. The **strictest model** was **anthropic/claude-haiku-4.5**, exhibiting a **36.4% refusal rate**, followed by **openai/gpt-5.1-codex-mini** at **32.0%**. Conversely, the **most permissible model** was **x-ai/grok-4.1-fast**, which demonstrated a **7.4% refusal rate**, indicating a far more lenient approach to content moderation. The disparity in refusal rates suggests varying adherence to safety protocols among the audited models.

Several **key disagreements** were noted where models returned different verdicts, pointing to potential inconsistencies in content interpretation and appropriateness. Understanding these differences can aid in refining guidelines and improving uniformity in moderation standards across models. This variability highlights the ongoing challenge of establishing cohesive safety criteria that align across multiple AI systems.

Overall, the **"Safety Vibe"** of the week suggests a cautious landscape, with stark contrasts in refusal rates emphasizing the need for further investigation into the underlying moderation frameworks employed by each model. This underscores the critical importance of aligning AI outputs with safety and ethical standards consistently.