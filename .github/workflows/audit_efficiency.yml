name: Audit (Efficiency Tier)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 4 1,15 * *' # Run on 1st and 15th of month (Bi-weekly ish)

# Prevents multiple runs from conflicting
concurrency:
  group: efficiency-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 60 
    permissions:
      contents: write 

    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0 

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Set up Node.js
        uses: actions/setup-node@v6
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          echo "üì¶ Installed Packages:"
          pip list


      - name: Run Unit Tests
        env:
          OPENROUTER_API_KEY: "mock_key_for_ci"
          PYTHONPATH: .
        run: |
          python -m pytest -W ignore::DeprecationWarning

      - name: Run Model Collection (Low Tier)
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          PYTHONPATH: .
        run: |
          # Use --preset low for efficiency run
          MAX_ATTEMPTS=3
          ATTEMPT=1
          DELAY=30
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "=== Attempt $ATTEMPT of $MAX_ATTEMPTS ==="
            
            if python src/audit_runner.py --preset low --resolve-latest; then
              echo "‚úÖ Model collection completed successfully"
              exit 0
            else
              echo "‚ùå Attempt $ATTEMPT failed"
              if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
                sleep $DELAY
                DELAY=$((DELAY * 2))
              fi
            fi
            ATTEMPT=$((ATTEMPT + 1))
          done
          
      - name: Compress Data
        run: python scripts/compress_data.py

      - name: Generate All Reports & Visuals
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          PYTHONPATH: .
        run: bash scripts/generate_all_reports.sh

      - name: Commit and Push Results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git stash push -m "New analysis data" --include-untracked
          git pull --rebase origin main
          git stash pop || echo "Warning: Stash pop reported conflicts."
          
          # Add compressed, ignore raw
          git add web/public/audit_log.csv.gz audit.db
          # Ensure raw file is untracked 
          git rm --cached web/public/audit_log.csv || true
          
          # Add everything else
          git add .
          
          if ! git diff --cached --quiet; then
            git commit -m "Auto-update: Low Tier Audit for $(date +%Y-%m-%d) [compressed]"
            git push origin main
          else
            echo "No changes to commit"
          fi

      - name: Deploy to Vercel
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
        run: |
          mkdir -p web/public/data
          cp audit_log.csv web/public/
          cp audit.db web/audit.db
          if [ -f "data/latest_report.md" ]; then
            cp data/latest_report.md web/public/
          fi
          npx vercel deploy --prod --token=$VERCEL_TOKEN --yes

      - name: Sync to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          pip install huggingface_hub datasets pandas
          python scripts/sync_to_huggingface.py
