name: Audit (Efficiency Tier)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 4 * * 0' # Run every Sunday at 4am UTC (Weekly)

# Prevents multiple runs from conflicting
concurrency:
  group: efficiency-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 360 
    permissions:
      contents: write 

    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0 

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Set up Node.js
        uses: actions/setup-node@v6
        with:
          node-version: 20

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          echo "ðŸ“¦ Installed Packages:"
          pip list


      - name: Run Unit Tests
        env:
          OPENROUTER_API_KEY: "mock_key_for_ci"
          PYTHONPATH: .
        run: |
          python -m pytest -W ignore::DeprecationWarning

      - name: Run Model Collection (Low Tier)
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          PYTHONPATH: .
        run: |
          # Use --preset low for efficiency run
          MAX_ATTEMPTS=3
          ATTEMPT=1
          DELAY=30
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "=== Attempt $ATTEMPT of $MAX_ATTEMPTS ==="
            
            if python src/audit_runner.py --preset low --resolve-latest --consistency 1; then
              echo "âœ… Model collection completed successfully"
              exit 0
            else
              echo "âŒ Attempt $ATTEMPT failed"
              if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
                sleep $DELAY
                DELAY=$((DELAY * 2))
              fi
            fi
            ATTEMPT=$((ATTEMPT + 1))
          done
          
      - name: Compress Data
        run: python scripts/compress_data.py

      - name: Generate All Reports & Visuals
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          PYTHONPATH: .
        run: bash scripts/generate_all_reports.sh

      - name: Commit and Push Results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git stash push -m "New analysis data" --include-untracked
          git pull --rebase origin main
          git stash pop || echo "Warning: Stash pop reported conflicts."
          
          # Add compressed data only (audit.db is too large for GitHub)
          git add web/public/audit_log.csv.gz
          # Ensure raw file and large db are untracked 
          git rm --cached web/public/audit_log.csv || true
          git rm --cached audit.db 2>/dev/null || true
          
          # Add everything else
          git add .
          
          if ! git diff --cached --quiet; then
            git commit -m "Auto-update: Low Tier Audit for $(date +%Y-%m-%d) [compressed]"
            git push origin main
          else
            echo "No changes to commit"
          fi

      - name: Deploy to Vercel
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
        run: |
          mkdir -p web/public/data
          # Copy compressed or uncompressed audit log (whichever exists)
          if [ -f "web/public/audit_log.csv.gz" ]; then
            echo "Using compressed audit_log.csv.gz"
          elif [ -f "audit_log.csv" ]; then
            cp audit_log.csv web/public/
          elif [ -f "web/public/audit_log.csv" ]; then
            echo "audit_log.csv already in place"
          else
            echo "Warning: No audit_log.csv found, skipping"
          fi
          # Copy audit.db if it exists
          if [ -f "audit.db" ]; then
            cp audit.db web/audit.db
          else
            echo "Warning: audit.db not found, skipping"
          fi
          if [ -f "data/latest_report.md" ]; then
            cp data/latest_report.md web/public/
          fi
          npx vercel deploy --prod --token=$VERCEL_TOKEN --yes

      - name: Sync to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          pip install huggingface_hub datasets pandas
          python scripts/sync_to_huggingface.py
