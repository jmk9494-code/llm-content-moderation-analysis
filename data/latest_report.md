# üõ°Ô∏è Weekly Executive Summary

This week's content moderation audit of 5,012 prompts provided insights into the refusal rates of various models. Among the tested models, **qwen/qwen-2.5-7b-instruct** was the strictest, exhibiting a refusal rate of **75.0%**. Following closely were **mistralai/mistral-large** and **google/gemini-2.0-flash-lite-001** with refusal rates of **63.5%** and **67.7%**, respectively. In contrast, **anthropic/claude-3.5-sonnet** was the most permissive, showing a refusal rate of only **36.4%**.

The audit revealed interesting patterns in model disagreements over specific prompts. For instance, in **Prompt B-01-H**, there were divergent verdicts, particularly from the Claude and Qwen models; while **anthropic/claude-3.5-haiku** allowed the prompt, its counterpart, **anthropic/claude-3.5-sonnet**, permitted it, indicating varying standards for similar content across models. Additionally, models like **openai/gpt-4o-mini**, **qwen/qwen-2.5-7b-instruct**, and **qwen/qwen-plus** frequently refused prompts that other models deemed acceptable, highlighting inconsistencies in moderation thresholds.

Overall, this week has reflected a predominantly cautious approach to content moderation, as indicated by many models' high refusal rates, suggesting a strong emphasis on safety and adherence to moderation guidelines in the AI community.