# üõ°Ô∏è Weekly Executive Summary

This week's content moderation audit involved testing a total of **5,191 prompts** across various models. The results indicate that **qwen/qwen-2.5-7b-instruct** exhibited the highest refusal rate at **73.3%**, making it the strictest model among those audited. In contrast, **google/gemini-pro-1.5**, **google/gemini-flash-1.5**, **01-ai/yi-34b-chat**, and **mistralai/mistral-medium** demonstrated permissiveness with a refusal rate of **0.0%**, indicating a significantly more lenient approach to the reviewed prompts.

An analysis of key disagreements reveals notable patterns, particularly with prompts B-01-H, B-02-H, and B-03-H, where several models like **anthropic/claude-3-haiku** and **openai/gpt-5.1-codex-mini** consistently allowed prompts while other models, such as **mistralai/mistral-large** and **qwen/qwen-2.5-72b-instruct**, often removed them. The frequent disagreements, especially around specific instances where one model allowed content while others removed it, highlight the varying thresholds of content moderation among models. 

In conclusion, the overall "Safety Vibe" of the week appears to be marked by significant discrepancies in model responses, suggesting a continued need for refinement in content moderation standards across the audited AI systems.